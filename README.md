# LLM-based Functional Coverage Generation and Auto-Evaluation Framework
Scripts for generation and evaluation of functional coverage within
Python using LLMs. Currently only LLMs that are run via Ollama server
are supported. 

Project is written with focus on extensibility, therefore to add another
test data, you should only need to extend the dataset folder and it should
work out of the box. There might be a possibility that you might need to edit
the port discovery code logic in `testbench.py` (see globals `*_NAMES`).

**DISCLAIMER:** Project is just a part of a prototype stored within
a private repository, and will be updated sparingly whenever some
milestone is reached.


## Setup
 1. Install `Python3.13`.
 1. Install `Verilator` (tested on version `5.036 (2025-04-27)`).
 1. Create virtual environment: `python3.13 -m venv venv`.
 1. Source the environment: `source venv/bin/activate`.
 1. Install the requirements: `pip install -r requirements.txt`.
 1. Compile the DPI for coverage: `sh compile_dpi.sh` 
    (might require update paths in the script).


## Generation and Evaluation
First create and run `Ollama` server with a model of your choice according
to the ([tutorial](https://ollama.readthedocs.io/en/quickstart). We rocommend
to use the docker or podman image. If you run the server remotely, you can
create an SSH tunnel to your local machine with:
`ssh -f -N -L 11434:localhost:11434 [ssh server]`.

Than run the pipeline using: `python pipeline.py [model name] -o [result file]`.
The model name must exactly match the name used in the `Ollama` server. First,
all the requirements will be processed to generate functional coverage, and
then the simulation for each intividual attempt will be run. 

If you are interested in how the generation works, feel free to explore the
`generation.py` file. Shortly, it works as follows:
 1. Each requirement is taken and pasted into a chat including a prompt template.
 1. LLM generates some python code based on that requirement and the prompt.
 1. The static analysis is performed and any issues are put back into the chat.
 1. The process is repeated until the static analysis passes or three attempts 
    are exceed.
 1. When all the code is generated a simulation proceed where a universal
    testbench takes all bathches of functional coverage code and runs a
    simulation using them one by one.

The final results can be summarized using: `python summary.py [result dir]`.
Where the result dir contains all the result files generated by the pipeline.


## Project Structure
There are following directories:

| Name            | Description                                      |
|-----------------|--------------------------------------------------|
| `dataset/`      | Contains DUTs, specifications, and requirements. |
| `results/`      | Stores all results from the generation pipeline. |
| `test_results/` | Final results from the evaluation pipeline.      |


And following scripts:

| Name             | Description                                              |
|------------------|----------------------------------------------------------|
| `coverage.py`    | Implementatino of the functional coverage for CoCoTB.    |
| `dataset.py`     | Loader for the dataset.                                  |
| `generation.py`  | Logic and templates handling the code generation.        |
| `pipeline.py`    | Script that runs the generation pipieline.               |
| `summary.py`     | Summarization of the generated and evaluated data.       |


## Dataset
The input into the framework is dataset consisting of verification
requirements coupled with the desired functional coverage implied by
them and the designs. The `top` must be unique among all instances!
Specification to each design is stored in `[top].md`. In addition, each
design contains port and code related to the statement coverage capture
with comments that ensure they are excluded in the computed statement
coverage.

The format of the dataset is:
```json
[
  {
    "top": "(str) Name of the top module, design file is than '{top}.sv'",
    "requirements": [
      {
        "description": "(str) description of the verification requirement",
        "coverpoints": [
          {
            "port": "(str) name of the port."
            "values": [
              {
                "all": "([int, int]) Require all values from given range."
              },
              {
                "one": "([int, int]) Require at least one value in given range."
              }
            ]
            "illegal_values": "Illegal values defined in the same way as 'values'."
            "sequences": "(list[list[int]]) Targeted sequences for the port.",
            "at_least": "(int) Optional, describes the required number of hits."
          },
          {
            "cross": [
              {
                "port": "(str) Name of the port."
                "values": "See 'coverpoints.values' above."
              }
            ]
          }
        ]
      }
    ]
  }
]
```
